{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuerde no agregar o quitar celdas en este notebook, ni modificar su tipo. Si lo hace, el sistema automaticamente lo calificará con cero punto cero (0.0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenga las letras asociadas a cada clave para el siguiente archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "0 \t C,F,A,B,D,I,H\n",
    "1 \t A,H,C,I,F,D,B\n",
    "2 \t B,H,I,F,G\n",
    "3 \t C,B,D\n",
    "4 \t D,C,I,G,H\n",
    "5 \t B,D,C,H,A\n",
    "6 \t H,D,C,B,G,F,D\n",
    "7 \t F,A,G,C,B,D,H,I\n",
    "8 \t F,A,I,B,D\n",
    "9 \t F,A,B,D,C,D,G,I\n",
    "10 \t D,B,A,C,H\n",
    "11 \t G,D,B,H,I,C,F\n",
    "12 \t D,D,C,F,B,A,H,G\n",
    "13 \t F,A,D\n",
    "14 \t D,A,C,G\n",
    "15 \t H,A,F,D,B,C,G,I\n",
    "16 \t A,I,B,D\n",
    "17 \t C,B,G,A,D,H,F\n",
    "18 \t I,B,A,H,D,F\n",
    "19 \t B,D,F,D,I\n",
    "20 \t C,B,H,F,I,G,D,D\n",
    "21 \t F,A,B,C,G,D\n",
    "22 \t I,G,F,C,H,B\n",
    "23 \t H,F,C,B,D,D,A\n",
    "24 \t F,D,G,A,H\n",
    "25 \t G,H,B,C,A,F,I\n",
    "26 \t G,F,B,A,H,D,D,I\n",
    "27 \t B,A,H,I,D,G,F\n",
    "28 \t A,H,D,F,C\n",
    "29 \t C,D,A,F,G,B,H,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys\n",
    "if __name__ == \"__main__\": \n",
    "    for line in sys.stdin:\n",
    "        splitLine = line.split()\n",
    "        key = splitLine[0]\n",
    "        \n",
    "        # Generate Pairs\n",
    "        for valueItem in splitLine[1].split(','):\n",
    "            sys.stdout.write( \"{}\\t{}\\n\".format(valueItem, key) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__': \n",
    "    \n",
    "    currentKey = None\n",
    "    currentValueArr = []\n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        rowKey, rowValue = line.split(\"\\t\")\n",
    "        if rowKey == currentKey:\n",
    "            for item in rowValue.strip().split(\",\"):\n",
    "                currentValueArr.append(item)\n",
    "        else:\n",
    "            if currentKey is not None:\n",
    "                sys.stdout.write( \"{}\\t{}\\n\".format(currentKey, \",\".join(currentValueArr)) )\n",
    "            currentKey = rowKey\n",
    "            currentValueArr = rowValue.strip().split(\",\")\n",
    "            \n",
    "    sys.stdout.write( \"{}\\t{}\\n\".format(currentKey, \",\".join(currentValueArr)) )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t1,18,26,21,10,7,0,12,25,8,27,13,14,15,24,29,16,28,23,9,17,5\n",
      "B\t22,19,20,21,1,29,2,3,5,6,7,27,8,9,26,11,12,0,25,15,16,17,23,18,10\n",
      "C\t0,29,28,25,23,22,21,20,17,15,14,12,11,10,9,7,6,5,4,3,1\n",
      "D\t19,19,18,17,23,23,1,16,24,15,14,13,29,12,12,11,10,9,9,26,26,8,7,27,6,0,6,5,4,3,28,21,20,20,29\n",
      "F\t18,25,6,20,9,0,1,23,11,8,24,27,28,17,21,19,13,29,26,12,22,7,15,2\n",
      "G\t22,6,27,26,11,25,2,9,20,24,14,21,15,12,29,17,7,4\n",
      "H\t25,12,1,11,10,26,0,2,20,27,7,22,6,18,23,17,5,28,4,24,15,29\n",
      "I\t7,16,25,15,8,26,2,9,11,4,22,18,20,0,27,19,1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) Server VM warning: You have loaded library /home/cespedesz07/hadoop-3.1.1/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.\n",
      "It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.\n",
      "2018-10-27 01:26:24,164 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-10-27 01:26:25,338 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-10-27 01:26:25,489 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-10-27 01:26:25,489 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-10-27 01:26:25,528 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-10-27 01:26:26,377 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2018-10-27 01:26:26,450 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2018-10-27 01:26:26,786 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local812462433_0001\n",
      "2018-10-27 01:26:26,791 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-10-27 01:26:27,200 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-10-27 01:26:27,205 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-10-27 01:26:27,205 INFO mapreduce.Job: Running job: job_local812462433_0001\n",
      "2018-10-27 01:26:27,209 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2018-10-27 01:26:27,228 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-10-27 01:26:27,229 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-10-27 01:26:27,349 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-10-27 01:26:27,360 INFO mapred.LocalJobRunner: Starting task: attempt_local812462433_0001_m_000000_0\n",
      "2018-10-27 01:26:27,436 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-10-27 01:26:27,441 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-10-27 01:26:27,492 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-10-27 01:26:27,510 INFO mapred.MapTask: Processing split: file:/home/cespedesz07/Documentos/Projects/mod-0-hadoop-cespedesz07/input.txt:0+507\n",
      "2018-10-27 01:26:27,532 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-10-27 01:26:27,648 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-10-27 01:26:27,648 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-10-27 01:26:27,648 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-10-27 01:26:27,649 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-10-27 01:26:27,649 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-10-27 01:26:27,654 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-10-27 01:26:27,665 INFO streaming.PipeMapRed: PipeMapRed exec [/home/cespedesz07/Documentos/Projects/mod-0-hadoop-cespedesz07/./mapper.py]\n",
      "2018-10-27 01:26:27,677 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2018-10-27 01:26:27,678 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2018-10-27 01:26:27,679 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2018-10-27 01:26:27,679 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2018-10-27 01:26:27,681 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2018-10-27 01:26:27,681 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2018-10-27 01:26:27,682 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2018-10-27 01:26:27,683 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-10-27 01:26:27,684 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2018-10-27 01:26:27,685 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2018-10-27 01:26:27,686 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-10-27 01:26:27,686 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2018-10-27 01:26:27,730 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 01:26:27,732 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 01:26:27,783 INFO streaming.PipeMapRed: Records R/W=30/1\n",
      "2018-10-27 01:26:27,788 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-10-27 01:26:27,792 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-10-27 01:26:27,798 INFO mapred.LocalJobRunner: \n",
      "2018-10-27 01:26:27,798 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-10-27 01:26:27,798 INFO mapred.MapTask: Spilling map output\n",
      "2018-10-27 01:26:27,798 INFO mapred.MapTask: bufstart = 0; bufend = 860; bufvoid = 104857600\n",
      "2018-10-27 01:26:27,798 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213664(104854656); length = 733/6553600\n",
      "2018-10-27 01:26:27,835 INFO mapred.MapTask: Finished spill 0\n",
      "2018-10-27 01:26:27,869 INFO mapred.Task: Task:attempt_local812462433_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-10-27 01:26:27,875 INFO mapred.LocalJobRunner: Records R/W=30/1\n",
      "2018-10-27 01:26:27,875 INFO mapred.Task: Task 'attempt_local812462433_0001_m_000000_0' done.\n",
      "2018-10-27 01:26:27,899 INFO mapred.Task: Final Counters for attempt_local812462433_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177003\n",
      "\t\tFILE: Number of bytes written=683392\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=184\n",
      "\t\tMap output bytes=860\n",
      "\t\tMap output materialized bytes=1234\n",
      "\t\tInput split bytes=129\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=184\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=214433792\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=507\n",
      "2018-10-27 01:26:27,900 INFO mapred.LocalJobRunner: Finishing task: attempt_local812462433_0001_m_000000_0\n",
      "2018-10-27 01:26:27,902 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-10-27 01:26:27,911 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-10-27 01:26:27,911 INFO mapred.LocalJobRunner: Starting task: attempt_local812462433_0001_r_000000_0\n",
      "2018-10-27 01:26:27,934 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-10-27 01:26:27,934 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-10-27 01:26:27,936 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-10-27 01:26:27,945 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1806960\n",
      "2018-10-27 01:26:27,951 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-10-27 01:26:28,000 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=605552640, maxSingleShuffleLimit=151388160, mergeThreshold=399664768, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-10-27 01:26:28,004 INFO reduce.EventFetcher: attempt_local812462433_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-10-27 01:26:28,060 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local812462433_0001_m_000000_0 decomp: 1230 len: 1234 to MEMORY\n",
      "2018-10-27 01:26:28,067 INFO reduce.InMemoryMapOutput: Read 1230 bytes from map-output for attempt_local812462433_0001_m_000000_0\n",
      "2018-10-27 01:26:28,070 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1230, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1230\n",
      "2018-10-27 01:26:28,073 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-10-27 01:26:28,074 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-10-27 01:26:28,075 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-10-27 01:26:28,095 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-10-27 01:26:28,095 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1226 bytes\n",
      "2018-10-27 01:26:28,111 INFO reduce.MergeManagerImpl: Merged 1 segments, 1230 bytes to disk to satisfy reduce memory limit\n",
      "2018-10-27 01:26:28,113 INFO reduce.MergeManagerImpl: Merging 1 files, 1234 bytes from disk\n",
      "2018-10-27 01:26:28,114 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-10-27 01:26:28,114 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-10-27 01:26:28,115 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1226 bytes\n",
      "2018-10-27 01:26:28,116 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-10-27 01:26:28,124 INFO streaming.PipeMapRed: PipeMapRed exec [/home/cespedesz07/Documentos/Projects/mod-0-hadoop-cespedesz07/./reducer.py]\n",
      "2018-10-27 01:26:28,131 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-10-27 01:26:28,132 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-10-27 01:26:28,195 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 01:26:28,196 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 01:26:28,200 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 01:26:28,218 INFO mapreduce.Job: Job job_local812462433_0001 running in uber mode : false\n",
      "2018-10-27 01:26:28,220 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2018-10-27 01:26:28,237 INFO streaming.PipeMapRed: Records R/W=184/1\n",
      "2018-10-27 01:26:28,241 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-10-27 01:26:28,242 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-10-27 01:26:28,243 INFO mapred.Task: Task:attempt_local812462433_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-10-27 01:26:28,245 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-10-27 01:26:28,245 INFO mapred.Task: Task attempt_local812462433_0001_r_000000_0 is allowed to commit now\n",
      "2018-10-27 01:26:28,248 INFO output.FileOutputCommitter: Saved output of task 'attempt_local812462433_0001_r_000000_0' to file:/home/cespedesz07/Documentos/Projects/mod-0-hadoop-cespedesz07/output\n",
      "2018-10-27 01:26:28,249 INFO mapred.LocalJobRunner: Records R/W=184/1 > reduce\n",
      "2018-10-27 01:26:28,250 INFO mapred.Task: Task 'attempt_local812462433_0001_r_000000_0' done.\n",
      "2018-10-27 01:26:28,251 INFO mapred.Task: Final Counters for attempt_local812462433_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=179503\n",
      "\t\tFILE: Number of bytes written=685146\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1234\n",
      "\t\tReduce input records=184\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=19\n",
      "\t\tTotal committed heap usage (bytes)=214433792\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=520\n",
      "2018-10-27 01:26:28,252 INFO mapred.LocalJobRunner: Finishing task: attempt_local812462433_0001_r_000000_0\n",
      "2018-10-27 01:26:28,252 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-10-27 01:26:29,222 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-10-27 01:26:29,222 INFO mapreduce.Job: Job job_local812462433_0001 completed successfully\n",
      "2018-10-27 01:26:29,242 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=356506\n",
      "\t\tFILE: Number of bytes written=1368538\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=184\n",
      "\t\tMap output bytes=860\n",
      "\t\tMap output materialized bytes=1234\n",
      "\t\tInput split bytes=129\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1234\n",
      "\t\tReduce input records=184\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=368\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=19\n",
      "\t\tTotal committed heap usage (bytes)=428867584\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=507\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=520\n",
      "2018-10-27 01:26:29,242 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf output\n",
    "STREAM=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar\n",
    "chmod +x mapper.py\n",
    "chmod +x reducer.py\n",
    "$HADOOP_HOME/bin/hadoop jar $STREAM -input input.txt -output output  -mapper mapper.py -reducer reducer.py\n",
    "cat output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mapper.py reducer.py output input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la evaluación automática de este libro:\n",
    "\n",
    "* Abra un Terminal.\n",
    "* Asegurece que esat en la misma carpeta que contiene este notebook.\n",
    "* Salve el notebook.\n",
    "* Ejecute el siguiente comando:\n",
    "\n",
    "      ./gradetool 11-Taller.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
